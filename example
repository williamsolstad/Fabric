# -*- coding: utf-8 -*-
# Pub-Aqua Sites → LH_ClientDemo.A_Bronze (full JSON → Delta)

from __future__ import annotations
import json
from datetime import datetime, timezone
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union

import requests
from requests.adapters import HTTPAdapter

try:
    from urllib3.util.retry import Retry
    _RETRY_KW = dict(allowed_methods=frozenset(["GET"]))
except Exception:
    from urllib3.util.retry import Retry  # type: ignore
    _RETRY_KW = dict(method_whitelist=frozenset(["GET"]))  # type: ignore

from notebookutils import mssparkutils
from pyspark.sql import Column, SparkSession, functions as F
from pyspark.sql.utils import AnalysisException

from pub_aqua_geo import SiteBorderClient

# -----------------------
# Configuration
# -----------------------
BASE_URL         = "https://api.fiskeridir.no/pub-aqua"
TIMEOUT          = (10, 60)
USER_AGENT       = "pub-aqua-sites-client/2.1"
MAX_RETRIES      = 5
BACKOFF_FACTOR   = 1.5
STATUS_FORCELIST = (429, 500, 502, 503, 504)

# IMPORTANT: your lakehouse/database name
LAKEHOUSE_DB     = "LH_ClientDemo"          # <— change only if your lakehouse is named differently
# --- CONFIG (2-part names; requires the notebook to have LH_ClientDemo attached) ---
BRONZE_SCHEMA          = "A_Bronze"
SITES_TABLE_NAME       = "PubAqua_Sites_Raw"
ENTITIES_TABLE_NAME    = "PubAqua_Entities_Raw"
SITES_BRONZE_TABLE     = f"`{BRONZE_SCHEMA}`.`{SITES_TABLE_NAME}`"     # e.g. A_Bronze.PubAqua_Sites_Raw
ENTITIES_BRONZE_TABLE  = f"`{BRONZE_SCHEMA}`.`{ENTITIES_TABLE_NAME}`"

# -----------------------
# HTTP session with retry
# -----------------------
def _build_session() -> requests.Session:
    s = requests.Session()
    retry = Retry(
        total=MAX_RETRIES,
        read=MAX_RETRIES,
        connect=MAX_RETRIES,
        backoff_factor=BACKOFF_FACTOR,
        status_forcelist=STATUS_FORCELIST,
        raise_on_status=False,
        respect_retry_after_header=True,
        **_RETRY_KW,
    )
    s.mount("https://", HTTPAdapter(max_retries=retry, pool_connections=30, pool_maxsize=30))
    s.headers.update({"User-Agent": USER_AGENT, "Accept": "application/json"})
    return s

SESSION = _build_session()
BORDER_CLIENT = SiteBorderClient(
    session=SESSION,
    base_url=BASE_URL,
    timeout=TIMEOUT,
    parse_response=_parse_or_raise,
)

# -----------------------
# Helpers
# -----------------------
def _iso8601_z(dt: Union[str, datetime]) -> str:
    if isinstance(dt, str):
        return dt
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

def _build_query(
    nr: Optional[str] = None,
    legal_entity_nr_id: Optional[str] = None,
    legal_entity_nr: Optional[str] = None,
    license_nr: Optional[str] = None,
    name: Optional[str] = None,
    placement: Optional[str] = None,
    water: Optional[str] = None,
    species_type: Optional[str] = None,
    municipality_code: Optional[str] = None,
    county_code: Optional[str] = None,
    production_area_code: Optional[str] = None,
    valid_from: Optional[Union[str, datetime]] = None,
    registered_from: Optional[Union[str, datetime]] = None,
    temporary_until: Optional[str] = None,
    activity: Optional[str] = None,
    operation: Optional[str] = None,
    range_: Optional[str] = None,
) -> Dict[str, Any]:
    q: Dict[str, Any] = {}
    if nr: q["nr"] = nr
    if legal_entity_nr_id: q["legal-entity-nr-id"] = legal_entity_nr_id
    if legal_entity_nr: q["legal-entity-nr"] = legal_entity_nr
    if license_nr: q["license-nr"] = license_nr
    if name: q["name"] = name
    if placement: q["placement"] = placement
    if water: q["water"] = water
    if species_type: q["species-type"] = species_type
    if municipality_code: q["municipality-code"] = municipality_code
    if county_code: q["county-code"] = county_code
    if production_area_code: q["production-area-code"] = production_area_code
    if valid_from: q["valid-from"] = _iso8601_z(valid_from)
    if registered_from: q["registered-from"] = _iso8601_z(registered_from)
    if temporary_until: q["temporary-until"] = temporary_until
    if activity: q["activity"] = activity
    if operation: q["operation"] = operation
    if range_: q["range"] = range_
    return q

def _parse_or_raise(resp: requests.Response) -> List[Dict[str, Any]]:
    if resp.status_code >= 400:
        try:
            err = resp.json()
        except Exception:
            err = resp.text
        raise requests.HTTPError(f"{resp.status_code} {resp.reason} for {resp.url} :: {err}")
    try:
        data = resp.json()
    except ValueError:
        return []
    if isinstance(data, dict) and isinstance(data.get("data"), list):
        return data["data"]
    if isinstance(data, list):
        return data
    return []

# -----------------------
# API (single + iterator)
# -----------------------
def fetch_sites_once(**filters) -> List[Dict[str, Any]]:
    params = _build_query(**filters)
    url = f"{BASE_URL}/api/v1/sites"
    resp = SESSION.get(url, params=params, timeout=TIMEOUT)
    return _parse_or_raise(resp)

def iter_sites(page_size: int = 100, max_pages: Optional[int] = None, **filters) -> Iterable[Tuple[int, List[Dict[str, Any]]]]:
    if page_size > 100:
        page_size = 100
    k = 0
    while True:
        if max_pages is not None and k >= max_pages:
            return
        start = k * page_size
        end   = start + page_size - 1
        batch = fetch_sites_once(range_=f"{start}-{end}", **filters)
        if not batch:
            return
        yield k, batch
        if len(batch) < page_size:
            return
        k += 1


# -----------------------
# Lakehouse helpers
# -----------------------
def _ensure_bronze_schema() -> None:
    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    # Rely on the attached lakehouse; no USE CATALOG / USE <db>
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS `{BRONZE_SCHEMA}`")

def _json_batch_to_df(spark: SparkSession, batch: List[Dict[str, Any]]):
    json_rows = [json.dumps(r, ensure_ascii=False) for r in batch]
    return spark.read.json(spark.sparkContext.parallelize(json_rows))


ColumnBuilder = Callable[["DataFrame"], Column]


def _col(name: str) -> ColumnBuilder:
    def builder(df: "DataFrame") -> Column:
        return F.col(name)

    return builder


def _first_address_field(field_name: str) -> ColumnBuilder:
    def builder(df: "DataFrame") -> Column:
        return F.col("addresses").getItem(0).getField(field_name)

    return builder


def _first_connection_field(field_name: str) -> ColumnBuilder:
    def builder(df: "DataFrame") -> Column:
        connections = F.col("connections")
        return F.when(F.size(connections) > 0, connections.getItem(0).getField(field_name))

    return builder


def _select_columns(
    df: "DataFrame",
    columns: List[Tuple[str, ColumnBuilder]],
) -> "DataFrame":
    selected: List[Column] = []
    for alias, builder in columns:
        try:
            candidate = builder(df)
            df.select(candidate)
        except AnalysisException:
            continue
        selected.append(candidate.alias(alias))

    if not selected:
        return df
    return df.select(*selected)


SITES_TABLE_COLUMNS: List[Tuple[str, ColumnBuilder]] = [
    ("siteNr", _col("siteNr")),
    ("name", _col("name")),
    ("licenseNr", _first_connection_field("licenseNr")),
    (
        "activity",
        lambda df: F.when(
            F.col("hasCommercialActivity").isNull(),
            F.lit(None),
        ).when(
            F.col("hasCommercialActivity"),
            F.lit("COMMERCIAL"),
        ).otherwise(F.lit("NON_COMMERCIAL")),
    ),
    (
        "operation",
        lambda df: F.when(
            F.col("hasJointOperation").isNull(),
            F.lit(None),
        ).when(
            F.col("hasJointOperation"),
            F.lit("JOINT"),
        ).otherwise(F.lit("SOLO")),
    ),
    ("placement", _col("placement")),
    ("placementType", _col("placementTypeValue")),
    ("water", _col("waterTypeValue")),
    ("municipalityCode", _col("placement.municipalityCode")),
    ("municipalityName", _col("placement.municipalityName")),
    ("countyCode", _col("placement.countyCode")),
    ("countyName", _col("placement.countyName")),
    ("productionAreaCode", _col("placement.prodAreaCode")),
    ("productionAreaName", _col("placement.prodAreaName")),
]


ENTITIES_TABLE_COLUMNS: List[Tuple[str, ColumnBuilder]] = [
    ("entityId", lambda df: F.col("id")),
    ("versionId", _col("versionId")),
    ("name", _col("name")),
    ("typeName", _col("typeName")),
    ("openNr", _col("openNr")),
    ("primaryZipCode", _first_address_field("zipCode")),
    ("primaryCountyName", _first_address_field("countyName")),
]


def _sites_batch_to_table_df(spark: SparkSession, batch: List[Dict[str, Any]]):
    raw_df = _json_batch_to_df(spark, batch)
    return _select_columns(raw_df, SITES_TABLE_COLUMNS)


def _entities_batch_to_table_df(spark: SparkSession, batch: List[Dict[str, Any]]):
    raw_df = _json_batch_to_df(spark, batch)
    return _select_columns(raw_df, ENTITIES_TABLE_COLUMNS)


def _write_batch_to_bronze(
    batch: List[Dict[str, Any]],
    table_name: str,
    transform: Callable[[SparkSession, List[Dict[str, Any]]], "DataFrame"],
) -> int:
    if not batch:
        return 0

    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    _ensure_bronze_schema()

    df = transform(spark, batch)
    df = df.withColumn("ingestion_ts", F.current_timestamp())

    # Append as managed Delta table under LH_ClientDemo ▸ A_Bronze
    (
        df.write
        .format("delta")
        .mode("append")
        .option("mergeSchema", "true")
        .saveAsTable(table_name)
    )

    return df.count()

def load_sites_to_bronze(page_size: int = 100, max_pages: Optional[int] = 1, **filters) -> int:
    total = 0
    for k, batch in iter_sites(page_size=page_size, max_pages=max_pages, **filters):
        written = _write_batch_to_bronze(batch, SITES_BRONZE_TABLE, _sites_batch_to_table_df)
        total += written
        print(f"Written page {k} ({written} rows) → {SITES_BRONZE_TABLE}")
    print(f"Total rows written → {SITES_BRONZE_TABLE}: {total}")
    return total

def _build_entities_query(
    entity_nr_id: Optional[str] = None,
    entity_nr: Optional[str] = None,
    name: Optional[str] = None,
    zip_code: Optional[str] = None,
    zip_name: Optional[str] = None,
    county_code: Optional[str] = None,
    county_name: Optional[str] = None,
    country_code: Optional[str] = None,
    country_name: Optional[str] = None,
    range_: Optional[str] = None,
) -> Dict[str, Any]:
    q: Dict[str, Any] = {}
    if entity_nr_id: q["entity-nr-id"] = entity_nr_id
    if entity_nr: q["entity-nr"] = entity_nr
    if name: q["name"] = name
    if zip_code: q["zip-code"] = zip_code
    if zip_name: q["zip-name"] = zip_name
    if county_code: q["county-code"] = county_code
    if county_name: q["county-name"] = county_name
    if country_code: q["country-code"] = country_code
    if country_name: q["country-name"] = country_name
    if range_: q["range"] = range_
    return q

def fetch_entities_once(**filters) -> List[Dict[str, Any]]:
    params = _build_entities_query(**filters)
    url = f"{BASE_URL}/api/v1/entities"
    resp = SESSION.get(url, params=params, timeout=TIMEOUT)
    return _parse_or_raise(resp)

def iter_entities(page_size: int = 100, max_pages: Optional[int] = None, **filters) -> Iterable[Tuple[int, List[Dict[str, Any]]]]:
    if page_size > 100:
        page_size = 100
    k = 0
    while True:
        if max_pages is not None and k >= max_pages:
            return
        start = k * page_size
        end   = start + page_size - 1
        batch = fetch_entities_once(range_=f"{start}-{end}", **filters)
        if not batch:
            return
        yield k, batch
        if len(batch) < page_size:
            return
        k += 1

def load_entities_to_bronze(page_size: int = 100, max_pages: Optional[int] = 1, **filters) -> int:
    total = 0
    for k, batch in iter_entities(page_size=page_size, max_pages=max_pages, **filters):
        written = _write_batch_to_bronze(
            batch,
            ENTITIES_BRONZE_TABLE,
            _entities_batch_to_table_df,
        )
        total += written
        print(f"Written page {k} ({written} rows) → {ENTITIES_BRONZE_TABLE}")
    print(f"Total rows written → {ENTITIES_BRONZE_TABLE}: {total}")
    return total

# -----------------------
# Run
# -----------------------
if __name__ == "__main__":
    total_sites = load_sites_to_bronze()

    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    print(f"Previewing the first rows from {SITES_BRONZE_TABLE} (total written: {total_sites})")
    sites_table = spark.table(SITES_BRONZE_TABLE)
    site_columns = [alias for alias, _ in SITES_TABLE_COLUMNS if alias in sites_table.columns]
    if site_columns:
        site_columns.append("ingestion_ts")
    else:
        site_columns = sites_table.columns
    sites_preview = sites_table.select(*site_columns)
    if "siteNr" in site_columns:
        sites_preview = sites_preview.orderBy(F.col("siteNr"))
    sites_preview = sites_preview.limit(25)

    try:
        display(sites_preview)
    except Exception:
        import pandas as pd
        from IPython.display import display as ipy_display
        ipy_display(sites_preview.toPandas())

    border_preview_rows: List[Tuple[Union[int, str], int, float, float]] = []
    if "siteNr" in site_columns:
        site_numbers = [row["siteNr"] for row in sites_preview.select("siteNr").limit(5).collect()]
        for site_nr in site_numbers:
            try:
                points = BORDER_CLIENT.fetch_site_border_points(site_nr)
            except requests.HTTPError as exc:
                print(f"Unable to fetch border points for site {site_nr}: {exc}")
                continue
            for idx, (latitude, longitude) in enumerate(points):
                border_preview_rows.append((site_nr, idx, latitude, longitude))

    if border_preview_rows:
        border_preview_df = spark.createDataFrame(
            border_preview_rows,
            schema=["siteNr", "pointIndex", "latitude", "longitude"],
        )
        print("Previewing the first border points (limit 5 sites)")
        try:
            display(border_preview_df)
        except Exception:
            import pandas as pd
            from IPython.display import display as ipy_display

            ipy_display(border_preview_df.toPandas())

    total_entities = load_entities_to_bronze()
    print(f"Previewing the first rows from {ENTITIES_BRONZE_TABLE} (total written: {total_entities})")
    entities_table = spark.table(ENTITIES_BRONZE_TABLE)
    entity_columns = [alias for alias, _ in ENTITIES_TABLE_COLUMNS if alias in entities_table.columns]
    if entity_columns:
        entity_columns.append("ingestion_ts")
    else:
        entity_columns = entities_table.columns
    entities_preview = entities_table.select(*entity_columns)
    if "entityId" in entity_columns:
        entities_preview = entities_preview.orderBy(F.col("entityId"))
    entities_preview = entities_preview.limit(25)

    try:
        display(entities_preview)
    except Exception:
        import pandas as pd
        from IPython.display import display as ipy_display
        ipy_display(entities_preview.toPandas())
