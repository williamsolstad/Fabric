# -*- coding: utf-8 -*-
"""
Fiskeridirektoratet pub-aqua full fetcher (Python/PySpark)
- Fetches all listed endpoints from https://api.fiskeridir.no/pub-aqua
- Robust retries + parallel fan-out for detail endpoints
- Writes JSONL dumps (always) and Parquet (if Spark is available)

Author: You
"""

import os
import sys
import json
import time
import math
import gzip
import uuid
import queue
import shutil
import threading
from typing import Any, Dict, Iterable, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

# ---------------------------
# Optional: Spark bootstrap
# ---------------------------
USE_SPARK = True
spark = None
try:
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.getOrCreate()
except Exception:
    USE_SPARK = False
    spark = None

# ---------------------------
# HTTP client with retries
# ---------------------------
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

BASE_URL = "https://api.fiskeridir.no/pub-aqua"
USER_AGENT = "pub-aqua-fetcher/1.0 (+for-powerbi)"
TIMEOUT = (10, 60)  # (connect, read)
MAX_RETRIES = 5
BACKOFF_FACTOR = 1.5
STATUS_FORCELIST = (429, 500, 502, 503, 504)
ALLOWED_METHODS = frozenset(["GET"])

def build_session() -> requests.Session:
    s = requests.Session()
    retry = Retry(
        total=MAX_RETRIES,
        read=MAX_RETRIES,
        connect=MAX_RETRIES,
        backoff_factor=BACKOFF_FACTOR,
        status_forcelist=STATUS_FORCELIST,
        allowed_methods=ALLOWED_METHODS,
        raise_on_status=False,
        respect_retry_after_header=True
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)
    s.mount("https://", adapter)
    s.headers.update({
        "User-Agent": USER_AGENT,
        "Accept": "application/json"
    })
    return s

SESSION = build_session()

def GET(path: str, params: Optional[Dict[str, Any]] = None) -> Any:
    """
    GET helper that returns parsed JSON or raises for hard failures.
    """
    url = f"{BASE_URL}{path}"
    r = SESSION.get(url, params=params or {}, timeout=TIMEOUT)
    # Handle unexpected non-JSON gracefully
    ctype = r.headers.get("Content-Type", "")
    if r.status_code >= 400:
        # Let 404 on child endpoints be soft-handled by caller if needed
        if r.status_code == 404:
            return None
        r.raise_for_status()
    if "application/json" not in ctype:
        # Try to parse anyway; if not, return raw text
        try:
            return r.json()
        except Exception:
            return r.text
    return r.json()

# ---------------------------
# Utilities
# ---------------------------
def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)

def jsonl_dump(records: Iterable[Dict[str, Any]], out_path: str) -> int:
    """
    Writes iterable of dicts to JSONL. Returns count written.
    """
    ensure_dir(os.path.dirname(out_path))
    n = 0
    with open(out_path, "w", encoding="utf-8") as f:
        for rec in records:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            n += 1
    return n

def to_spark(df_name: str, rows: List[Dict[str, Any]], out_dir: str) -> Optional[int]:
    """
    Creates a Spark DataFrame from list of dicts and writes as Parquet.
    Returns row count if Spark is available.
    """
    if not USE_SPARK or spark is None:
        return None
    if not rows:
        # Create an empty DF with a placeholder schema to maintain table presence
        sdf = spark.createDataFrame([], schema="placeholder string")
    else:
        sdf = spark.createDataFrame(rows)
    path = os.path.join(out_dir, df_name)
    ensure_dir(path)
    sdf.write.mode("overwrite").parquet(path)
    return sdf.count()

def detect_first_key(d: Dict[str, Any], candidates: List[str]) -> Optional[str]:
    for k in candidates:
        if k in d:
            return k
    return None

def extract_id(record: Dict[str, Any], candidates: List[str]) -> Optional[Any]:
    key = detect_first_key(record, candidates)
    return record.get(key) if key else None

def parallel_map(func, items: List[Any], max_workers: int = 16, desc: str = "") -> List[Any]:
    out: List[Any] = [None] * len(items)
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {ex.submit(func, idx, item): idx for idx, item in enumerate(items)}
        for fut in as_completed(futures):
            idx = futures[fut]
            try:
                out[idx] = fut.result()
            except Exception as e:
                print(f"[WARN] {desc} item {idx} failed: {e}", file=sys.stderr)
                out[idx] = None
    return out

# ---------------------------
# Fetchers for each domain
# ---------------------------

def fetch_sites_list() -> List[Dict[str, Any]]:
    # GET /api/v1/sites
    data = GET("/api/v1/sites")
    if isinstance(data, dict) and "data" in data and isinstance(data["data"], list):
        return data["data"]
    if isinstance(data, list):
        return data
    # Unknown shape; coerce into list
    return [data] if data else []

def fetch_site_detail(site_nr: Any) -> Dict[str, Any]:
    # GET /api/v1/sites/{site-nr}
    d = GET(f"/api/v1/sites/{site_nr}") or {}
    d["site_nr"] = site_nr
    return d

def fetch_site_license_connections(site_nr: Any) -> List[Dict[str, Any]]:
    # GET /api/v1/sites/{site-nr}/license-connections
    data = GET(f"/api/v1/sites/{site_nr}/license-connections")
    rows = data if isinstance(data, list) else (data or [])
    for r in rows:
        r["site_nr"] = site_nr
    return rows

def fetch_site_decisions(site_nr: Any) -> List[Dict[str, Any]]:
    # GET /api/v1/sites/{site-nr}/decisions
    data = GET(f"/api/v1/sites/{site_nr}/decisions")
    rows = data if isinstance(data, list) else (data or [])
    for r in rows:
        r["site_nr"] = site_nr
    return rows

def fetch_site_borders(site_nr: Any) -> Dict[str, Any]:
    # GET /api/v1/sites/{site-nr}/borders
    data = GET(f"/api/v1/sites/{site_nr}/borders") or {}
    # Some APIs return GeoJSON FeatureCollection | Feature
    return {"site_nr": site_nr, "borders": data}

def fetch_licenses_list() -> List[Dict[str, Any]]:
    data = GET("/api/v1/licenses")
    if isinstance(data, dict) and "data" in data and isinstance(data["data"], list):
        return data["data"]
    if isinstance(data, list):
        return data
    return [data] if data else []

def fetch_licenses_overview_list() -> List[Dict[str, Any]]:
    data = GET("/api/v1/licenses-overview")
    if isinstance(data, dict) and "data" in data and isinstance(data["data"], list):
        return data["data"]
    if isinstance(data, list):
        return data
    return [data] if data else []

def fetch_license_detail(license_nr: Any) -> Dict[str, Any]:
    data = GET(f"/api/v1/licenses/{license_nr}") or {}
    data["license_nr"] = license_nr
    return data

def fetch_license_capacity_history(license_nr: Any) -> List[Dict[str, Any]]:
    data = GET(f"/api/v1/licenses/{license_nr}/capacity-history")
    rows = data if isinstance(data, list) else (data or [])
    for r in rows:
        r["license_nr"] = license_nr
    return rows

def fetch_license_site_connections(license_nr: Any) -> List[Dict[str, Any]]:
    data = GET(f"/api/v1/licenses/{license_nr}/site-connections")
    rows = data if isinstance(data, list) else (data or [])
    for r in rows:
        r["license_nr"] = license_nr
    return rows

def fetch_license_types_intentions() -> List[Dict[str, Any]]:
    data = GET("/api/v1/license-types/intentions")
    if isinstance(data, dict) and "data" in data and isinstance(data["data"], list):
        return data["data"]
    if isinstance(data, list):
        return data
    return [data] if data else []

def fetch_entities_list() -> List[Dict[str, Any]]:
    data = GET("/api/v1/entities")
    if isinstance(data, dict) and "data" in data and isinstance(data["data"], list):
        return data["data"]
    if isinstance(data, list):
        return data
    return [data] if data else []

def fetch_entity_detail(entity_id: Any) -> Dict[str, Any]:
    data = GET(f"/api/v1/entities/{entity_id}") or {}
    data["entity_id"] = entity_id
    return data

def fetch_entity_sites_by_id(entity_id: Any) -> List[Dict[str, Any]]:
    data = GET(f"/api/v1/entities/{entity_id}/sites")
    rows = data if isinstance(data, list) else (data or [])
    for r in rows:
        r["entity_id"] = entity_id
    return rows

def fetch_entity_sites_by_entity_nr(entity_nr: Any) -> List[Dict[str, Any]]:
    data = GET(f"/api/v1/entities/sites-by-entity-nr/{entity_nr}")
    rows = data if isinstance(data, list) else (data or [])
    for r in rows:
        r["entity_nr"] = entity_nr
    return rows

def fetch_areas_list() -> List[Dict[str, Any]]:
    data = GET("/api/v1/areas")
    if isinstance(data, dict) and "data" in data and isinstance(data["data"], list):
        return data["data"]
    if isinstance(data, list):
        return data
    return [data] if data else []

def fetch_area_detail(area_type: str, code: str) -> Dict[str, Any]:
    data = GET(f"/api/v1/areas/{area_type}/{code}") or {}
    data["area_type"] = area_type
    data["code"] = code
    return data

# ---------------------------
# Orchestration
# ---------------------------

def main(output_root: str = "./pub_aqua_out", max_workers: int = 16) -> None:
    ensure_dir(output_root)

    # 1) Sites and children
    print("Fetching sites list …")
    sites = fetch_sites_list()
    print(f"Sites: {len(sites)}")

    # Detect site id key
    site_id_key = None
    if sites:
        site_id_key = detect_first_key(
            sites[0],
            ["siteNr", "site-nr", "siteNumber", "site_no", "localityNo", "lokalitetsnummer"]
        ) or "siteNr"
    print(f"Detected site id key: {site_id_key}")

    site_ids = [extract_id(x, [site_id_key]) for x in sites if extract_id(x, [site_id_key]) is not None]

    # Fan-out: site details
    print("Fetching site details …")
    site_details = [x for x in parallel_map(lambda i, s: fetch_site_detail(s), site_ids, max_workers, desc="site-detail") if x]

    print("Fetching site license connections …")
    site_license_conns_all: List[Dict[str, Any]] = []
    for chunk_idx, chunk in enumerate([site_ids[i:i+200] for i in range(0, len(site_ids), 200)]):
        res = parallel_map(lambda i, s: fetch_site_license_connections(s), chunk, max_workers, desc=f"site-license-conns-{chunk_idx}")
        for r in res:
            if r:
                site_license_conns_all.extend(r)

    print("Fetching site decisions …")
    site_decisions_all: List[Dict[str, Any]] = []
    for chunk_idx, chunk in enumerate([site_ids[i:i+200] for i in range(0, len(site_ids), 200)]):
        res = parallel_map(lambda i, s: fetch_site_decisions(s), chunk, max_workers, desc=f"site-decisions-{chunk_idx}")
        for r in res:
            if r:
                site_decisions_all.extend(r)

    print("Fetching site borders (geometry) …")
    site_borders = [x for x in parallel_map(lambda i, s: fetch_site_borders(s), site_ids, max_workers, desc="site-borders") if x]

    # 2) Licenses and children
    print("Fetching licenses list …")
    licenses = fetch_licenses_list()
    print(f"Licenses: {len(licenses)}")

    print("Fetching licenses overview list …")
    licenses_overview = fetch_licenses_overview_list()
    print(f"Licenses overview: {len(licenses_overview)}")

    license_id_key = None
    if licenses:
        license_id_key = detect_first_key(
            licenses[0],
            ["licenseNr", "license-nr", "licenseNumber", "licensenr", "l_n"]
        ) or "licenseNr"
    print(f"Detected license id key: {license_id_key}")

    license_ids = [extract_id(x, [license_id_key]) for x in licenses if extract_id(x, [license_id_key]) is not None]

    print("Fetching license details …")
    license_details = [x for x in parallel_map(lambda i, l: fetch_license_detail(l), license_ids, max_workers, desc="license-detail") if x]

    print("Fetching license capacity history …")
    license_capacity_hist_all: List[Dict[str, Any]] = []
    for chunk_idx, chunk in enumerate([license_ids[i:i+200] for i in range(0, len(license_ids), 200)]):
        res = parallel_map(lambda i, l: fetch_license_capacity_history(l), chunk, max_workers, desc=f"license-capacity-{chunk_idx}")
        for r in res:
            if r:
                license_capacity_hist_all.extend(r)

    print("Fetching license site-connections …")
    license_site_conns_all: List[Dict[str, Any]] = []
    for chunk_idx, chunk in enumerate([license_ids[i:i+200] for i in range(0, len(license_ids), 200)]):
        res = parallel_map(lambda i, l: fetch_license_site_connections(l), chunk, max_workers, desc=f"license-site-conns-{chunk_idx}")
        for r in res:
            if r:
                license_site_conns_all.extend(r)

    print("Fetching license types intentions …")
    license_types_intentions = fetch_license_types_intentions()

    # 3) Entities and children
    print("Fetching entities list …")
    entities = fetch_entities_list()
    print(f"Entities: {len(entities)}")

    entity_id_key = None
    entity_nr_key = None
    if entities:
        entity_id_key = detect_first_key(entities[0], ["id", "entityId"]) or "id"
        entity_nr_key = detect_first_key(entities[0], ["entityNr", "entity-nr", "entityNumber", "orgnr"]) or "entityNr"
    print(f"Detected entity id key: {entity_id_key}, entity nr key: {entity_nr_key}")

    entity_ids = [extract_id(x, [entity_id_key]) for x in entities if extract_id(x, [entity_id_key]) is not None]
    entity_nrs = [extract_id(x, [entity_nr_key]) for x in entities if extract_id(x, [entity_nr_key]) is not None]

    print("Fetching entity details …")
    entity_details = [x for x in parallel_map(lambda i, e: fetch_entity_detail(e), entity_ids, max_workers, desc="entity-detail") if x]

    print("Fetching entity sites by entity id …")
    entity_sites_by_id_all: List[Dict[str, Any]] = []
    for chunk_idx, chunk in enumerate([entity_ids[i:i+200] for i in range(0, len(entity_ids), 200)]):
        res = parallel_map(lambda i, e: fetch_entity_sites_by_id(e), chunk, max_workers, desc=f"entity-sites-id-{chunk_idx}")
        for r in res:
            if r:
                entity_sites_by_id_all.extend(r)

    print("Fetching entity sites by entity nr …")
    entity_sites_by_nr_all: List[Dict[str, Any]] = []
    for chunk_idx, chunk in enumerate([entity_nrs[i:i+200] for i in range(0, len(entity_nrs), 200)]):
        res = parallel_map(lambda i, en: fetch_entity_sites_by_entity_nr(en), chunk, max_workers, desc=f"entity-sites-nr-{chunk_idx}")
        for r in res:
            if r:
                entity_sites_by_nr_all.extend(r)

    # 4) Areas and details
    print("Fetching areas list …")
    areas = fetch_areas_list()
    print(f"Areas entries: {len(areas)}")

    # Area item might have different keys; try to detect
    area_type_key = None
    area_code_key = None
    if areas:
        area_type_key = detect_first_key(areas[0], ["type", "areaType", "kategori"]) or "type"
        area_code_key = detect_first_key(areas[0], ["code", "areaCode", "kode"]) or "code"
    print(f"Detected area keys -> type: {area_type_key}, code: {area_code_key}")

    dedup_area_pairs = []
    for a in areas:
        t = a.get(area_type_key)
        c = a.get(area_code_key)
        if t is not None and c is not None:
            dedup_area_pairs.append((str(t), str(c)))
    dedup_area_pairs = sorted(list(set(dedup_area_pairs)))

    print("Fetching area details …")
    area_details = [x for x in parallel_map(lambda i, tc: fetch_area_detail(tc[0], tc[1]), dedup_area_pairs, max_workers, desc="area-detail") if x]

    # ---------------------------
    # Persist: JSONL (raw) + Spark Parquet (curated)
    # ---------------------------
    raw_dir = os.path.join(output_root, "raw_jsonl")
    curated_dir = os.path.join(output_root, "parquet")

    tables = {
        "sites": sites,
        "site_details": site_details,
        "site_license_connections": site_license_conns_all,
        "site_decisions": site_decisions_all,
        "site_borders": site_borders,

        "licenses": licenses,
        "licenses_overview": licenses_overview,
        "license_details": license_details,
        "license_capacity_history": license_capacity_hist_all,
        "license_site_connections": license_site_conns_all,
        "license_types_intentions": license_types_intentions,

        "entities": entities,
        "entity_details": entity_details,
        "entity_sites_by_id": entity_sites_by_id_all,
        "entity_sites_by_entity_nr": entity_sites_by_nr_all,

        "areas": areas,
        "area_details": area_details,
    }

    print("\nWriting JSONL dumps …")
    for name, rows in tables.items():
        out_path = os.path.join(raw_dir, f"{name}.jsonl")
        cnt = jsonl_dump(rows, out_path)
        print(f"  - {name:<30} {cnt:>8} rows -> {out_path}")

    if USE_SPARK and spark is not None:
        print("\nWriting Parquet tables (Spark) …")
        for name, rows in tables.items():
            try:
                n = to_spark(name, rows, curated_dir)
                print(f"  - {name:<30} {n if n is not None else 0:>8} rows -> {os.path.join(curated_dir, name)}")
            except Exception as e:
                print(f"[WARN] Spark write failed for {name}: {e}", file=sys.stderr)
    else:
        print("\n[INFO] Spark not available; skipped Parquet writes. JSONL files are ready.")

    print("\nDone.")

if __name__ == "__main__":
    # Tune these if needed
    OUTPUT_ROOT = os.environ.get("PUB_AQUA_OUT", "./pub_aqua_out")
    MAX_WORKERS = int(os.environ.get("PUB_AQUA_WORKERS", "16"))
    main(output_root=OUTPUT_ROOT, max_workers=MAX_WORKERS)
