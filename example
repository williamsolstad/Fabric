# -*- coding: utf-8 -*-
# Pub-Aqua Sites → LH_ClientDemo.A_Bronze (full JSON → Delta)

from __future__ import annotations
import json
from datetime import datetime, timezone
from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union

import requests
from requests.adapters import HTTPAdapter

try:
    from urllib3.util.retry import Retry
    _RETRY_KW = dict(allowed_methods=frozenset(["GET"]))
except Exception:
    from urllib3.util.retry import Retry  # type: ignore
    _RETRY_KW = dict(method_whitelist=frozenset(["GET"]))  # type: ignore

from notebookutils import mssparkutils
from pyspark.sql import Column, SparkSession, functions as F, types as T
from pyspark.sql.utils import AnalysisException

# -----------------------
# Geo client (inlined)
# -----------------------


class SiteBorderClient:
    """Small helper that fetches site border points from the Pub-Aqua API.

    The original example depends on a ``pub_aqua_geo`` package that ships the
    real client.  The execution environment for this kata does not have that
    package installed which means importing it results in a
    :class:`ModuleNotFoundError`.  To keep the example self contained we inline
    a lightweight drop-in replacement that covers the bits of behaviour the
    notebook relies on.

    The implementation intentionally focuses on robustness instead of strictly
    mimicking the upstream module.  The API occasionally returns slightly
    different payload shapes (plain ``{"latitude": .., "longitude": ..}``,
    ``{"lat": .., "lon": ..}``, GeoJSON ``geometry`` objects, ...).  The
    helpers below try to normalise all those formats into ``(lat, lon)`` tuples
    with ``float``/``None`` values so the rest of the notebook can stay
    unchanged.
    """

    def __init__(
        self,
        session: requests.Session,
        base_url: str,
        timeout: Union[int, float, Tuple[int, int]],
        parse_response: Callable[[requests.Response], List[Dict[str, Any]]],
    ) -> None:
        self._session = session
        self._base_url = base_url.rstrip("/")
        self._timeout = timeout
        self._parse_response = parse_response

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    def fetch_site_border_points(
        self, site_nr: Union[int, str]
    ) -> List[Tuple[Optional[float], Optional[float]]]:
        """Fetch border points for a given site number.

        The endpoint exposed by the service lives under
        ``<base_url>/geo/api/v1/sites/<site_nr>/border``.
        ``self._parse_response`` already handles HTTP error propagation and the
        ``data`` envelope, leaving this method with normalising the payload
        structure.
        """

        url = f"{self._base_url}/geo/api/v1/sites/{site_nr}/border"
        response = self._session.get(url, timeout=self._timeout)
        payload = self._parse_response(response)
        return self._normalise_payload(payload)

    # ------------------------------------------------------------------
    # Normalisation helpers
    # ------------------------------------------------------------------
    def _normalise_payload(
        self, payload: Iterable[Any]
    ) -> List[Tuple[Optional[float], Optional[float]]]:
        points: List[Tuple[Optional[float], Optional[float]]] = []
        for item in payload:
            points.extend(self._normalise_item(item))
        return points

    def _normalise_item(
        self, item: Any
    ) -> List[Tuple[Optional[float], Optional[float]]]:
        if item is None:
            return []

        if isinstance(item, dict):
            if "coordinates" in item:
                return self._coordinates_to_points(item["coordinates"])
            if "geometry" in item and isinstance(item["geometry"], dict):
                geometry = item["geometry"]
                return self._coordinates_to_points(geometry.get("coordinates"))

            lat = self._extract_number(item, ["latitude", "lat", "y"])
            lon = self._extract_number(item, ["longitude", "lon", "x", "lng"])
            if lat is not None or lon is not None:
                return [(lat, lon)]
            return []

        if isinstance(item, (list, tuple)):
            return self._coordinates_to_points(item)

        # Unsupported payload element – ignore it while keeping the client
        # resilient to future format changes.
        return []

    def _coordinates_to_points(
        self, coordinates: Any
    ) -> List[Tuple[Optional[float], Optional[float]]]:
        if coordinates is None:
            return []

        if isinstance(coordinates, (list, tuple)):
            if self._looks_like_coordinate_pair(coordinates):
                lat, lon = self._pair_to_lat_lon(coordinates)
                return [(lat, lon)]

            points: List[Tuple[Optional[float], Optional[float]]] = []
            for entry in coordinates:
                points.extend(self._coordinates_to_points(entry))
            return points

        # Anything else (e.g. dicts) gets routed back through the normaliser.
        return self._normalise_item(coordinates)

    # ------------------------------------------------------------------
    # Primitive helpers
    # ------------------------------------------------------------------
    @staticmethod
    def _extract_number(obj: Dict[str, Any], keys: Iterable[str]) -> Optional[float]:
        for key in keys:
            if key in obj:
                value = obj[key]
                if value is None:
                    return None
                try:
                    return float(value)
                except (TypeError, ValueError):
                    return None
        return None

    @staticmethod
    def _looks_like_coordinate_pair(values: Sequence[Any]) -> bool:
        if len(values) < 2:
            return False
        first, second = values[0], values[1]
        return SiteBorderClient._is_number(first) and SiteBorderClient._is_number(second)

    @staticmethod
    def _is_number(value: Any) -> bool:
        try:
            float(value)
        except (TypeError, ValueError):
            return False
        return True

    @staticmethod
    def _pair_to_lat_lon(values: Sequence[Any]) -> Tuple[Optional[float], Optional[float]]:
        first = float(values[0]) if values[0] is not None else None
        second = float(values[1]) if values[1] is not None else None
        lat, lon = first, second
        if lat is not None and (lat < -90 or lat > 90):
            # Likely lon/lat order (GeoJSON default).  Swap the components.
            lat, lon = lon, lat
        if lat is not None and (lat < -90 or lat > 90):
            lat = None
        if lon is not None and (lon < -180 or lon > 180):
            lon = None
        return lat, lon



# -----------------------
# Configuration
# -----------------------
BASE_URL         = "https://api.fiskeridir.no/pub-aqua"
TIMEOUT          = (10, 60)
USER_AGENT       = "pub-aqua-sites-client/2.1"
MAX_RETRIES      = 5
BACKOFF_FACTOR   = 1.5
STATUS_FORCELIST = (429, 500, 502, 503, 504)

# IMPORTANT: your lakehouse/database name
LAKEHOUSE_DB     = "LH_ClientDemo"          # <— change only if your lakehouse is named differently
# --- CONFIG (2-part names; requires the notebook to have LH_ClientDemo attached) ---
BRONZE_SCHEMA          = "A_Bronze"
SITES_TABLE_NAME       = "PubAqua_Sites_Raw"
ENTITIES_TABLE_NAME    = "PubAqua_Entities_Raw"
SITES_BRONZE_TABLE     = f"`{BRONZE_SCHEMA}`.`{SITES_TABLE_NAME}`"     # e.g. A_Bronze.PubAqua_Sites_Raw
ENTITIES_BRONZE_TABLE  = f"`{BRONZE_SCHEMA}`.`{ENTITIES_TABLE_NAME}`"
SITE_BORDER_POINTS_TABLE_NAME = "PubAqua_SiteBorderPoints_Raw"
SITE_BORDER_POINTS_BRONZE_TABLE = f"`{BRONZE_SCHEMA}`.`{SITE_BORDER_POINTS_TABLE_NAME}`"

# -----------------------
# HTTP session with retry
# -----------------------
def _build_session() -> requests.Session:
    s = requests.Session()
    retry = Retry(
        total=MAX_RETRIES,
        read=MAX_RETRIES,
        connect=MAX_RETRIES,
        backoff_factor=BACKOFF_FACTOR,
        status_forcelist=STATUS_FORCELIST,
        raise_on_status=False,
        respect_retry_after_header=True,
        **_RETRY_KW,
    )
    s.mount("https://", HTTPAdapter(max_retries=retry, pool_connections=30, pool_maxsize=30))
    s.headers.update({"User-Agent": USER_AGENT, "Accept": "application/json"})
    return s

# Forward declarations rely on helpers defined later in the file.  The API
# client is initialised after the response parser is declared.
SESSION = _build_session()

# -----------------------
# Helpers
# -----------------------
def _iso8601_z(dt: Union[str, datetime]) -> str:
    if isinstance(dt, str):
        return dt
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

def _build_query(
    nr: Optional[str] = None,
    legal_entity_nr_id: Optional[str] = None,
    legal_entity_nr: Optional[str] = None,
    license_nr: Optional[str] = None,
    name: Optional[str] = None,
    placement: Optional[str] = None,
    water: Optional[str] = None,
    species_type: Optional[str] = None,
    municipality_code: Optional[str] = None,
    county_code: Optional[str] = None,
    production_area_code: Optional[str] = None,
    valid_from: Optional[Union[str, datetime]] = None,
    registered_from: Optional[Union[str, datetime]] = None,
    temporary_until: Optional[str] = None,
    activity: Optional[str] = None,
    operation: Optional[str] = None,
    range_: Optional[str] = None,
) -> Dict[str, Any]:
    q: Dict[str, Any] = {}
    if nr: q["nr"] = nr
    if legal_entity_nr_id: q["legal-entity-nr-id"] = legal_entity_nr_id
    if legal_entity_nr: q["legal-entity-nr"] = legal_entity_nr
    if license_nr: q["license-nr"] = license_nr
    if name: q["name"] = name
    if placement: q["placement"] = placement
    if water: q["water"] = water
    if species_type: q["species-type"] = species_type
    if municipality_code: q["municipality-code"] = municipality_code
    if county_code: q["county-code"] = county_code
    if production_area_code: q["production-area-code"] = production_area_code
    if valid_from: q["valid-from"] = _iso8601_z(valid_from)
    if registered_from: q["registered-from"] = _iso8601_z(registered_from)
    if temporary_until: q["temporary-until"] = temporary_until
    if activity: q["activity"] = activity
    if operation: q["operation"] = operation
    if range_: q["range"] = range_
    return q

def _parse_or_raise(resp: requests.Response) -> List[Dict[str, Any]]:
    if resp.status_code >= 400:
        try:
            err = resp.json()
        except Exception:
            err = resp.text
        raise requests.HTTPError(f"{resp.status_code} {resp.reason} for {resp.url} :: {err}")
    try:
        data = resp.json()
    except ValueError:
        return []
    if isinstance(data, dict) and isinstance(data.get("data"), list):
        return data["data"]
    if isinstance(data, list):
        return data
    return []


BORDER_CLIENT = SiteBorderClient(
    session=SESSION,
    base_url=BASE_URL,
    timeout=TIMEOUT,
    parse_response=_parse_or_raise,
)

# -----------------------
# API (single + iterator)
# -----------------------
def fetch_sites_once(**filters) -> List[Dict[str, Any]]:
    params = _build_query(**filters)
    url = f"{BASE_URL}/api/v1/sites"
    resp = SESSION.get(url, params=params, timeout=TIMEOUT)
    return _parse_or_raise(resp)

def iter_sites(page_size: int = 100, max_pages: Optional[int] = None, **filters) -> Iterable[Tuple[int, List[Dict[str, Any]]]]:
    if page_size > 100:
        page_size = 100
    k = 0
    while True:
        if max_pages is not None and k >= max_pages:
            return
        start = k * page_size
        end   = start + page_size - 1
        batch = fetch_sites_once(range_=f"{start}-{end}", **filters)
        if not batch:
            return
        yield k, batch
        if len(batch) < page_size:
            return
        k += 1


# -----------------------
# Lakehouse helpers
# -----------------------
def _ensure_bronze_schema() -> None:
    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    # Rely on the attached lakehouse; no USE CATALOG / USE <db>
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS `{BRONZE_SCHEMA}`")

def _json_batch_to_df(spark: SparkSession, batch: List[Dict[str, Any]]):
    json_rows = [json.dumps(r, ensure_ascii=False) for r in batch]
    return spark.read.json(spark.sparkContext.parallelize(json_rows))


ColumnBuilder = Callable[["DataFrame"], Column]


def _col(name: str) -> ColumnBuilder:
    def builder(df: "DataFrame") -> Column:
        return F.col(name)

    return builder


PLACEMENT_FIELD_SPECS: List[Tuple[str, T.DataType]] = [
    ("municipalityCode", T.StringType()),
    ("municipalityName", T.StringType()),
    ("countyCode", T.StringType()),
    ("countyName", T.StringType()),
    ("prodAreaCode", T.StringType()),
    ("prodAreaName", T.StringType()),
    ("prodAreaStatus", T.StringType()),
]


def _first_address_field(field_name: str) -> ColumnBuilder:
    def builder(df: "DataFrame") -> Column:
        return F.col("addresses").getItem(0).getField(field_name)

    return builder


def _first_connection_field(field_name: str) -> ColumnBuilder:
    def builder(df: "DataFrame") -> Column:
        connections = F.col("connections")
        return F.when(F.size(connections) > 0, connections.getItem(0).getField(field_name))

    return builder


def _placement_struct(df: "DataFrame") -> Column:
    placement_field = next((field for field in df.schema if field.name == "placement"), None)
    if placement_field is None or not isinstance(placement_field.dataType, T.StructType):
        null_columns = [
            F.lit(None).cast(data_type).alias(name) for name, data_type in PLACEMENT_FIELD_SPECS
        ]
        return F.struct(*null_columns)

    placement_schema = placement_field.dataType
    assert isinstance(placement_schema, T.StructType)
    schema_field_names = {field.name for field in placement_schema}

    columns: List[Column] = []
    for name, data_type in PLACEMENT_FIELD_SPECS:
        if name in schema_field_names:
            columns.append(F.col(f"placement.{name}").cast(data_type).alias(name))
        else:
            columns.append(F.lit(None).cast(data_type).alias(name))

    return F.struct(*columns)


def _select_columns(
    df: "DataFrame",
    columns: List[Tuple[str, ColumnBuilder]],
) -> "DataFrame":
    selected: List[Column] = []
    for alias, builder in columns:
        try:
            candidate = builder(df)
            df.select(candidate)
        except AnalysisException:
            continue
        selected.append(candidate.alias(alias))

    if not selected:
        return df
    return df.select(*selected)


SITES_TABLE_COLUMNS: List[Tuple[str, ColumnBuilder]] = [
    ("siteNr", _col("siteNr")),
    ("name", _col("name")),
    ("licenseNr", _first_connection_field("licenseNr")),
    (
        "activity",
        lambda df: F.when(
            F.col("hasCommercialActivity").isNull(),
            F.lit(None),
        ).when(
            F.col("hasCommercialActivity"),
            F.lit("COMMERCIAL"),
        ).otherwise(F.lit("NON_COMMERCIAL")),
    ),
    (
        "operation",
        lambda df: F.when(
            F.col("hasJointOperation").isNull(),
            F.lit(None),
        ).when(
            F.col("hasJointOperation"),
            F.lit("JOINT"),
        ).otherwise(F.lit("SOLO")),
    ),
    ("placement", _placement_struct),
    ("placementType", _col("placementTypeValue")),
    ("water", _col("waterTypeValue")),
    ("municipalityCode", _col("placement.municipalityCode")),
    ("municipalityName", _col("placement.municipalityName")),
    ("countyCode", _col("placement.countyCode")),
    ("countyName", _col("placement.countyName")),
    ("productionAreaCode", _col("placement.prodAreaCode")),
    ("productionAreaName", _col("placement.prodAreaName")),
]


ENTITIES_TABLE_COLUMNS: List[Tuple[str, ColumnBuilder]] = [
    ("entityId", lambda df: F.col("id")),
    ("versionId", _col("versionId")),
    ("name", _col("name")),
    ("typeName", _col("typeName")),
    ("openNr", _col("openNr")),
    ("primaryZipCode", _first_address_field("zipCode")),
    ("primaryCountyName", _first_address_field("countyName")),
]


def _sites_batch_to_table_df(spark: SparkSession, batch: List[Dict[str, Any]]):
    raw_df = _json_batch_to_df(spark, batch)
    return _select_columns(raw_df, SITES_TABLE_COLUMNS)


def _entities_batch_to_table_df(spark: SparkSession, batch: List[Dict[str, Any]]):
    raw_df = _json_batch_to_df(spark, batch)
    return _select_columns(raw_df, ENTITIES_TABLE_COLUMNS)


def fetch_site_border_points(site_nr: Union[int, str]) -> List[Tuple[str, int, Optional[float], Optional[float]]]:
    try:
        points = BORDER_CLIENT.fetch_site_border_points(site_nr)
    except requests.HTTPError as exc:
        print(f"Unable to fetch border points for site {site_nr}: {exc}")
        return []

    rows: List[Tuple[str, int, Optional[float], Optional[float]]] = []
    site_identifier = str(site_nr)
    for idx, (latitude, longitude) in enumerate(points):
        lat_value = float(latitude) if latitude is not None else None
        lon_value = float(longitude) if longitude is not None else None
        rows.append((site_identifier, idx, lat_value, lon_value))
    return rows


def load_site_border_points_to_bronze(site_numbers: Iterable[Union[int, str]]) -> int:
    rows: List[Tuple[str, int, Optional[float], Optional[float]]] = []
    for site_nr in site_numbers:
        rows.extend(fetch_site_border_points(site_nr))

    if not rows:
        return 0

    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    _ensure_bronze_schema()

    df = spark.createDataFrame(
        rows,
        schema=["siteNr", "pointIndex", "latitude", "longitude"],
    )
    df = df.withColumn("ingestion_ts", F.current_timestamp())

    (
        df.write
        .format("delta")
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .saveAsTable(SITE_BORDER_POINTS_BRONZE_TABLE)
    )

    return df.count()


def _write_batch_to_bronze(
    batch: List[Dict[str, Any]],
    table_name: str,
    transform: Callable[[SparkSession, List[Dict[str, Any]]], "DataFrame"],
) -> int:
    if not batch:
        return 0

    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    _ensure_bronze_schema()

    df = transform(spark, batch)
    df = df.withColumn("ingestion_ts", F.current_timestamp())

    # Append as managed Delta table under LH_ClientDemo ▸ A_Bronze
    (
        df.write
        .format("delta")
        .mode("append")
        .option("mergeSchema", "true")
        .saveAsTable(table_name)
    )

    return df.count()

def load_sites_to_bronze(page_size: int = 100, max_pages: Optional[int] = 1, **filters) -> int:
    total = 0
    for k, batch in iter_sites(page_size=page_size, max_pages=max_pages, **filters):
        written = _write_batch_to_bronze(batch, SITES_BRONZE_TABLE, _sites_batch_to_table_df)
        total += written
        print(f"Written page {k} ({written} rows) → {SITES_BRONZE_TABLE}")
    print(f"Total rows written → {SITES_BRONZE_TABLE}: {total}")
    return total

def _build_entities_query(
    entity_nr_id: Optional[str] = None,
    entity_nr: Optional[str] = None,
    name: Optional[str] = None,
    zip_code: Optional[str] = None,
    zip_name: Optional[str] = None,
    county_code: Optional[str] = None,
    county_name: Optional[str] = None,
    country_code: Optional[str] = None,
    country_name: Optional[str] = None,
    range_: Optional[str] = None,
) -> Dict[str, Any]:
    q: Dict[str, Any] = {}
    if entity_nr_id: q["entity-nr-id"] = entity_nr_id
    if entity_nr: q["entity-nr"] = entity_nr
    if name: q["name"] = name
    if zip_code: q["zip-code"] = zip_code
    if zip_name: q["zip-name"] = zip_name
    if county_code: q["county-code"] = county_code
    if county_name: q["county-name"] = county_name
    if country_code: q["country-code"] = country_code
    if country_name: q["country-name"] = country_name
    if range_: q["range"] = range_
    return q

def fetch_entities_once(**filters) -> List[Dict[str, Any]]:
    params = _build_entities_query(**filters)
    url = f"{BASE_URL}/api/v1/entities"
    resp = SESSION.get(url, params=params, timeout=TIMEOUT)
    return _parse_or_raise(resp)

def iter_entities(page_size: int = 100, max_pages: Optional[int] = None, **filters) -> Iterable[Tuple[int, List[Dict[str, Any]]]]:
    if page_size > 100:
        page_size = 100
    k = 0
    while True:
        if max_pages is not None and k >= max_pages:
            return
        start = k * page_size
        end   = start + page_size - 1
        batch = fetch_entities_once(range_=f"{start}-{end}", **filters)
        if not batch:
            return
        yield k, batch
        if len(batch) < page_size:
            return
        k += 1

def load_entities_to_bronze(page_size: int = 100, max_pages: Optional[int] = 1, **filters) -> int:
    total = 0
    for k, batch in iter_entities(page_size=page_size, max_pages=max_pages, **filters):
        written = _write_batch_to_bronze(
            batch,
            ENTITIES_BRONZE_TABLE,
            _entities_batch_to_table_df,
        )
        total += written
        print(f"Written page {k} ({written} rows) → {ENTITIES_BRONZE_TABLE}")
    print(f"Total rows written → {ENTITIES_BRONZE_TABLE}: {total}")
    return total

# -----------------------
# Run
# -----------------------
if __name__ == "__main__":
    total_sites = load_sites_to_bronze()

    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    print(f"Previewing the first rows from {SITES_BRONZE_TABLE} (total written: {total_sites})")
    sites_table = spark.table(SITES_BRONZE_TABLE)
    site_columns = [alias for alias, _ in SITES_TABLE_COLUMNS if alias in sites_table.columns]
    if site_columns:
        site_columns.append("ingestion_ts")
    else:
        site_columns = sites_table.columns
    sites_preview = sites_table.select(*site_columns)
    if "siteNr" in site_columns:
        sites_preview = sites_preview.orderBy(F.col("siteNr"))
    sites_preview = sites_preview.limit(25)

    try:
        display(sites_preview)
    except Exception:
        import pandas as pd
        from IPython.display import display as ipy_display
        ipy_display(sites_preview.toPandas())

    border_preview_df = None
    if "siteNr" in site_columns:
        site_numbers = [row["siteNr"] for row in sites_preview.select("siteNr").limit(5).collect()]
        if site_numbers:
            written_border_points = load_site_border_points_to_bronze(site_numbers)
        else:
            written_border_points = 0
        if written_border_points:
            print(
                "Written border point rows "
                f"→ {SITE_BORDER_POINTS_BRONZE_TABLE}: {written_border_points}"
            )
            border_preview_df = (
                spark.table(SITE_BORDER_POINTS_BRONZE_TABLE)
                .orderBy(F.col("siteNr"), F.col("pointIndex"))
                .limit(25)
            )
        else:
            print("No border points written for preview.")
    else:
        print("No site numbers available to fetch border points.")
    if border_preview_df is not None:
        print("Previewing the first border points (limit 5 sites)")
        try:
            display(border_preview_df)
        except Exception:
            import pandas as pd
            from IPython.display import display as ipy_display

            ipy_display(border_preview_df.toPandas())

    total_entities = load_entities_to_bronze()
    print(f"Previewing the first rows from {ENTITIES_BRONZE_TABLE} (total written: {total_entities})")
    entities_table = spark.table(ENTITIES_BRONZE_TABLE)
    entity_columns = [alias for alias, _ in ENTITIES_TABLE_COLUMNS if alias in entities_table.columns]
    if entity_columns:
        entity_columns.append("ingestion_ts")
    else:
        entity_columns = entities_table.columns
    entities_preview = entities_table.select(*entity_columns)
    if "entityId" in entity_columns:
        entities_preview = entities_preview.orderBy(F.col("entityId"))
    entities_preview = entities_preview.limit(25)

    try:
        display(entities_preview)
    except Exception:
        import pandas as pd
        from IPython.display import display as ipy_display
        ipy_display(entities_preview.toPandas())
