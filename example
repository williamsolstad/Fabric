# -*- coding: utf-8 -*-
# Pub-Aqua Sites → LH_ClientDemo.A_Bronze (full JSON → Delta)

from __future__ import annotations
import json
from datetime import datetime, timezone
from typing import Any, Dict, Iterable, List, Optional, Tuple, Union

import requests
from requests.adapters import HTTPAdapter

try:
    from urllib3.util.retry import Retry
    _RETRY_KW = dict(allowed_methods=frozenset(["GET"]))
except Exception:
    from urllib3.util.retry import Retry  # type: ignore
    _RETRY_KW = dict(method_whitelist=frozenset(["GET"]))  # type: ignore

from notebookutils import mssparkutils
from pyspark.sql import SparkSession, functions as F, types as T

# -----------------------
# Configuration
# -----------------------
BASE_URL         = "https://api.fiskeridir.no/pub-aqua"
TIMEOUT          = (10, 60)
USER_AGENT       = "pub-aqua-sites-client/2.1"
MAX_RETRIES      = 5
BACKOFF_FACTOR   = 1.5
STATUS_FORCELIST = (429, 500, 502, 503, 504)

# IMPORTANT: your lakehouse/database name
LAKEHOUSE_DB     = "LH_ClientDemo"          # <— change only if your lakehouse is named differently
# --- CONFIG (2-part names; requires the notebook to have LH_ClientDemo attached) ---
BRONZE_SCHEMA          = "A_Bronze"
SITES_TABLE_NAME       = "PubAqua_Sites_Raw"
ENTITIES_TABLE_NAME    = "PubAqua_Entities_Raw"
SITES_BRONZE_TABLE     = f"`{BRONZE_SCHEMA}`.`{SITES_TABLE_NAME}`"     # e.g. A_Bronze.PubAqua_Sites_Raw
ENTITIES_BRONZE_TABLE  = f"`{BRONZE_SCHEMA}`.`{ENTITIES_TABLE_NAME}`"

# -----------------------
# HTTP session with retry
# -----------------------
def _build_session() -> requests.Session:
    s = requests.Session()
    retry = Retry(
        total=MAX_RETRIES,
        read=MAX_RETRIES,
        connect=MAX_RETRIES,
        backoff_factor=BACKOFF_FACTOR,
        status_forcelist=STATUS_FORCELIST,
        raise_on_status=False,
        respect_retry_after_header=True,
        **_RETRY_KW,
    )
    s.mount("https://", HTTPAdapter(max_retries=retry, pool_connections=30, pool_maxsize=30))
    s.headers.update({"User-Agent": USER_AGENT, "Accept": "application/json"})
    return s

SESSION = _build_session()

# -----------------------
# Helpers
# -----------------------
def _iso8601_z(dt: Union[str, datetime]) -> str:
    if isinstance(dt, str):
        return dt
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

def _build_query(
    nr: Optional[str] = None,
    legal_entity_nr_id: Optional[str] = None,
    legal_entity_nr: Optional[str] = None,
    license_nr: Optional[str] = None,
    name: Optional[str] = None,
    placement: Optional[str] = None,
    water: Optional[str] = None,
    species_type: Optional[str] = None,
    municipality_code: Optional[str] = None,
    county_code: Optional[str] = None,
    production_area_code: Optional[str] = None,
    valid_from: Optional[Union[str, datetime]] = None,
    registered_from: Optional[Union[str, datetime]] = None,
    temporary_until: Optional[str] = None,
    activity: Optional[str] = None,
    operation: Optional[str] = None,
    range_: Optional[str] = None,
) -> Dict[str, Any]:
    q: Dict[str, Any] = {}
    if nr: q["nr"] = nr
    if legal_entity_nr_id: q["legal-entity-nr-id"] = legal_entity_nr_id
    if legal_entity_nr: q["legal-entity-nr"] = legal_entity_nr
    if license_nr: q["license-nr"] = license_nr
    if name: q["name"] = name
    if placement: q["placement"] = placement
    if water: q["water"] = water
    if species_type: q["species-type"] = species_type
    if municipality_code: q["municipality-code"] = municipality_code
    if county_code: q["county-code"] = county_code
    if production_area_code: q["production-area-code"] = production_area_code
    if valid_from: q["valid-from"] = _iso8601_z(valid_from)
    if registered_from: q["registered-from"] = _iso8601_z(registered_from)
    if temporary_until: q["temporary-until"] = temporary_until
    if activity: q["activity"] = activity
    if operation: q["operation"] = operation
    if range_: q["range"] = range_
    return q

def _parse_or_raise(resp: requests.Response) -> List[Dict[str, Any]]:
    if resp.status_code >= 400:
        try:
            err = resp.json()
        except Exception:
            err = resp.text
        raise requests.HTTPError(f"{resp.status_code} {resp.reason} for {resp.url} :: {err}")
    try:
        data = resp.json()
    except ValueError:
        return []
    if isinstance(data, dict) and isinstance(data.get("data"), list):
        return data["data"]
    if isinstance(data, list):
        return data
    return []

# -----------------------
# API (single + iterator)
# -----------------------
def fetch_sites_once(**filters) -> List[Dict[str, Any]]:
    params = _build_query(**filters)
    url = f"{BASE_URL}/api/v1/sites"
    resp = SESSION.get(url, params=params, timeout=TIMEOUT)
    return _parse_or_raise(resp)

def iter_sites(page_size: int = 100, max_pages: Optional[int] = None, **filters) -> Iterable[Tuple[int, List[Dict[str, Any]]]]:
    if page_size > 100:
        page_size = 100
    k = 0
    while True:
        if max_pages is not None and k >= max_pages:
            return
        start = k * page_size
        end   = start + page_size - 1
        batch = fetch_sites_once(range_=f"{start}-{end}", **filters)
        if not batch:
            return
        yield k, batch
        if len(batch) < page_size:
            return
        k += 1

# -----------------------
# Lakehouse helpers
# -----------------------
def _ensure_bronze_schema() -> None:
    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    # Rely on the attached lakehouse; no USE CATALOG / USE <db>
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS `{BRONZE_SCHEMA}`")

def _write_batch_to_bronze(batch: List[Dict[str, Any]], table_name: str) -> int:
    if not batch:
        return 0

    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    _ensure_bronze_schema()

    json_rows = [(json.dumps(r, ensure_ascii=False),) for r in batch]
    df = spark.createDataFrame(json_rows, schema=T.StructType([T.StructField("payload", T.StringType(), False)]))
    df = df.withColumn("ingestion_ts", F.current_timestamp())

    # Append as managed Delta table under LH_ClientDemo ▸ A_Bronze
    (df.write
       .format("delta")
       .mode("append")
       .saveAsTable(table_name))

    return df.count()

def load_sites_to_bronze(page_size: int = 100, max_pages: Optional[int] = None, **filters) -> int:
    total = 0
    for k, batch in iter_sites(page_size=page_size, max_pages=max_pages, **filters):
        written = _write_batch_to_bronze(batch, SITES_BRONZE_TABLE)
        total += written
        print(f"Written page {k} ({written} rows) → {SITES_BRONZE_TABLE}")
    print(f"Total rows written → {SITES_BRONZE_TABLE}: {total}")
    return total

def _build_entities_query(
    entity_nr_id: Optional[str] = None,
    entity_nr: Optional[str] = None,
    name: Optional[str] = None,
    zip_code: Optional[str] = None,
    zip_name: Optional[str] = None,
    county_code: Optional[str] = None,
    county_name: Optional[str] = None,
    country_code: Optional[str] = None,
    country_name: Optional[str] = None,
    range_: Optional[str] = None,
) -> Dict[str, Any]:
    q: Dict[str, Any] = {}
    if entity_nr_id: q["entity-nr-id"] = entity_nr_id
    if entity_nr: q["entity-nr"] = entity_nr
    if name: q["name"] = name
    if zip_code: q["zip-code"] = zip_code
    if zip_name: q["zip-name"] = zip_name
    if county_code: q["county-code"] = county_code
    if county_name: q["county-name"] = county_name
    if country_code: q["country-code"] = country_code
    if country_name: q["country-name"] = country_name
    if range_: q["range"] = range_
    return q

def fetch_entities_once(**filters) -> List[Dict[str, Any]]:
    params = _build_entities_query(**filters)
    url = f"{BASE_URL}/api/v1/entities"
    resp = SESSION.get(url, params=params, timeout=TIMEOUT)
    return _parse_or_raise(resp)

def iter_entities(page_size: int = 100, max_pages: Optional[int] = None, **filters) -> Iterable[Tuple[int, List[Dict[str, Any]]]]:
    if page_size > 100:
        page_size = 100
    k = 0
    while True:
        if max_pages is not None and k >= max_pages:
            return
        start = k * page_size
        end   = start + page_size - 1
        batch = fetch_entities_once(range_=f"{start}-{end}", **filters)
        if not batch:
            return
        yield k, batch
        if len(batch) < page_size:
            return
        k += 1

def load_entities_to_bronze(page_size: int = 100, max_pages: Optional[int] = None, **filters) -> int:
    total = 0
    for k, batch in iter_entities(page_size=page_size, max_pages=max_pages, **filters):
        written = _write_batch_to_bronze(batch, ENTITIES_BRONZE_TABLE)
        total += written
        print(f"Written page {k} ({written} rows) → {ENTITIES_BRONZE_TABLE}")
    print(f"Total rows written → {ENTITIES_BRONZE_TABLE}: {total}")
    return total

# -----------------------
# Run
# -----------------------
if __name__ == "__main__":
    total_sites = load_sites_to_bronze()

    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    print(f"Previewing the first rows from {SITES_BRONZE_TABLE} (total written: {total_sites})")
    sites_preview = spark.sql(f"""
        SELECT
            get_json_object(payload, '$.siteNr')                 AS siteNr,
            get_json_object(payload, '$.name')                   AS name,
            get_json_object(payload, '$.water')                  AS water,
            get_json_object(payload, '$.productionAreaCode')     AS productionAreaCode,
            ingestion_ts
        FROM {SITES_BRONZE_TABLE}
        ORDER BY siteNr
        LIMIT 25
    """)

    try:
        display(sites_preview)
    except Exception:
        import pandas as pd
        from IPython.display import display as ipy_display
        ipy_display(sites_preview.toPandas())

    total_entities = load_entities_to_bronze()
    print(f"Previewing the first rows from {ENTITIES_BRONZE_TABLE} (total written: {total_entities})")
    entities_preview = spark.sql(f"""
        SELECT
            get_json_object(payload, '$.entityNr')           AS entityNr,
            get_json_object(payload, '$.entityNrId')         AS entityNrId,
            get_json_object(payload, '$.name')               AS name,
            get_json_object(payload, '$.zipCode')            AS zipCode,
            get_json_object(payload, '$.countyName')         AS countyName,
            ingestion_ts
        FROM {ENTITIES_BRONZE_TABLE}
        ORDER BY entityNr
        LIMIT 25
    """)

    try:
        display(entities_preview)
    except Exception:
        import pandas as pd
        from IPython.display import display as ipy_display
        ipy_display(entities_preview.toPandas())
